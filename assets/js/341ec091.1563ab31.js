"use strict";(self.webpackChunkjohnpottergr_github_io=self.webpackChunkjohnpottergr_github_io||[]).push([[9515],{3509:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>l});var i=n(5072),r=n(4848),s=n(8453);const a={title:"LLM Reverse Engineering with RAG to Probe AI Search Behavior",date:new Date("2025-06-26T00:00:00.000Z")},o=void 0,h={authorsImageUrls:[]},l=[{value:"What you can learned by rebuilding the retrieval stack",id:"what-you-can-learned-by-rebuilding-the-retrieval-stack",level:2},{value:"Why RAG Matters for Reverse Engineering",id:"why-rag-matters-for-reverse-engineering",level:2},{value:"What Comes Next",id:"what-comes-next",level:2}];function c(e){const t={em:"em",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:(0,r.jsx)(t.em,{children:"Note: RAG is a technique that improves an LLM's capabilities by integrating them with external data sources."})}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"RAG (Retrieval-Augmented Generation) systems"})," can be used to simulate and test how LLMs retrieve content. They don't just probe what a model knows, but what it displays when someone asks a question."]}),"\n",(0,r.jsx)("img",{src:"/img/rag.png",alt:"RAG",width:"800"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.em,{children:"Image From K21Academy.com"})}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"So here's a simple experiment."})," Use ChatGPT\u2019s search mode to run a query, copy the full text of the cited sources, and load them into a local RAG pipeline built with n8n. From there, start asking questions against the dataset and watch what the model pulls up. You can make small tweaks like changing headings, rephrasing sections, or adding new pages to see what ripple effects occur."]}),"\n",(0,r.jsx)(t.h2,{id:"what-you-can-learned-by-rebuilding-the-retrieval-stack",children:"What you can learned by rebuilding the retrieval stack"}),"\n",(0,r.jsx)(t.p,{children:"The first big insight here is that LLMs won\u2019t retrieve entire webpages, they will pull chunks... of paragraphs and sections. If the right terms aren\u2019t in the right block of text, the model won\u2019t select it\u2014even if the broader page is relevant."}),"\n",(0,r.jsx)(t.p,{children:"Next, you'll note that the Q&A structures perform really well even if they\u2019re not FAQs. If a page has summaries, question-like subheadings, or bullet-point answers, the model is much more likely to grab and reuse that text."}),"\n",(0,r.jsx)(t.p,{children:"But keywords still matter. Not so much across the page but in the chunks. If a keyword or phrase isn\u2019t embedded in the local context, an LLM might skip it entirely. In other words, the idea of keyword density is not the focus here."}),"\n",(0,r.jsx)(t.h2,{id:"why-rag-matters-for-reverse-engineering",children:"Why RAG Matters for Reverse Engineering"}),"\n",(0,r.jsx)(t.p,{children:"RAG  gives you a lab environment to test how AI might respond to real-world questions. You control the content and the prompt. You also get to see which chunks rise to the top."}),"\n",(0,r.jsx)(t.p,{children:"It\u2019s not a perfect mirror of live LLM behavior, but it\u2019s helpful enough to reveal certain patterns:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Which types of structure improve extractability"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"What phrasing improves retrievability"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"How small edits shift the model\u2019s attention"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"RAG is a smart way to test your content's influence without having to access the internal workings of a proprietary LLM."}),"\n",(0,r.jsx)(t.h2,{id:"what-comes-next",children:"What Comes Next"}),"\n",(0,r.jsx)(t.p,{children:"Retrievability is just as important as relevance here. You may write brilliant, on-topic content but still be invisible to AI if the structure doesn\u2019t fit with chunk-level extraction."}),"\n",(0,r.jsx)(t.p,{children:"Next up in this series, I\u2019ll look at how prompts, citations, and retrieval context shape what LLMs decide to surface\u2014and how that impacts your ability to show up in chat-based interfaces."})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},5072:e=>{e.exports=JSON.parse('{"permalink":"/blog/6-26-2025-embeddings-and-RAG","editUrl":"https://github.com/johnpottergr/johnpottergr.github.io/blog/6-26-2025-embeddings-and-RAG.md","source":"@site/blog/6-26-2025-embeddings-and-RAG.md","title":"LLM Reverse Engineering with RAG to Probe AI Search Behavior","description":"Note: RAG is a technique that improves an LLM\'s capabilities by integrating them with external data sources.","date":"2025-06-26T00:00:00.000Z","tags":[],"readingTime":2.09,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"LLM Reverse Engineering with RAG to Probe AI Search Behavior","date":"2025-06-26T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Building Online Authority Through Embeddings Intelligence","permalink":"/blog/6-27-2025-building-authority-through-embeddigs"},"nextItem":{"title":"LLM Reverse Engineering with Embeddings","permalink":"/blog/6-25-2025-reverse-engineering-with-embeddings"}}')},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(6540);const r={},s=i.createContext(r);function a(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);